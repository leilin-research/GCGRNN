{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "#from tensorflow.contrib.rnn.python.ops import rnn_cell as RNNCell\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "import collections\n",
    "from tensorflow.contrib import rnn\n",
    "import h5py\n",
    "#from tensorflow.python.ops.rnn_cell_impl import _RNNCell \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "\n",
    "def masked_mae_tf(preds, labels, null_val=np.nan):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    :param preds:\n",
    "    :param labels:\n",
    "    :param null_val:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #print (preds.shape)\n",
    "    #print (labels.shape)\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~tf.is_nan(labels)\n",
    "    else:\n",
    "        mask = tf.not_equal(labels, null_val)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    mask = tf.where(tf.is_nan(mask), tf.zeros_like(mask), mask)\n",
    "    loss = tf.abs(tf.subtract(preds, labels))\n",
    "    loss = loss * mask\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_mae_tf_by_horizon(preds, labels, null_val=np.nan):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    :param preds:\n",
    "    :param labels:\n",
    "    :param null_val:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    preds_reshape = tf.reshape(preds, [-1, sn, horizon])\n",
    "    labels_reshape = tf.reshape(labels, [-1, sn, horizon])\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        labels = labels_reshape[:, :, 0:(i+1)]\n",
    "        preds = preds_reshape[:, :, 0:(i+1)]\n",
    "        \n",
    "        if np.isnan(null_val):\n",
    "            mask = ~tf.is_nan(labels)\n",
    "        else:\n",
    "            mask = tf.not_equal(labels, null_val)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        mask = tf.where(tf.is_nan(mask), tf.zeros_like(mask), mask)\n",
    "        loss = tf.abs(tf.subtract(preds, labels))\n",
    "        loss = loss * mask\n",
    "        loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "        \n",
    "        res.append(tf.reduce_mean(loss))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tables\n",
    "raw_data = pd.read_hdf('../../data/METR-LA/metr-la.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34272, 207)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:00:00</th>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.125</td>\n",
       "      <td>67.125</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.625000</td>\n",
       "      <td>65.500</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>66.428571</td>\n",
       "      <td>66.875</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>61.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:05:00</th>\n",
       "      <td>62.666667</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>68.111111</td>\n",
       "      <td>65.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>57.444444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>69.875</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.555556</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>55.888889</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>62.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:10:00</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500</td>\n",
       "      <td>64.250</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:15:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:20:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        773869     767541     767542     717447     717446  \\\n",
       "2012-03-01 00:00:00  64.375000  67.625000  67.125000  61.500000  66.875000   \n",
       "2012-03-01 00:05:00  62.666667  68.555556  65.444444  62.444444  64.444444   \n",
       "2012-03-01 00:10:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "                        717445  773062  767620     737529     717816  ...  \\\n",
       "2012-03-01 00:00:00  68.750000  65.125  67.125  59.625000  62.750000  ...   \n",
       "2012-03-01 00:05:00  68.111111  65.000  65.000  57.444444  63.333333  ...   \n",
       "2012-03-01 00:10:00  66.250000  64.500  64.250  63.875000  65.375000  ...   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "\n",
       "                        772167  769372     774204     769806  717590  \\\n",
       "2012-03-01 00:00:00  45.625000  65.500  64.500000  66.428571  66.875   \n",
       "2012-03-01 00:05:00  50.666667  69.875  66.666667  58.555556  62.000   \n",
       "2012-03-01 00:10:00  44.125000  69.000  56.500000  59.250000  68.125   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "\n",
       "                        717592     717595     772168     718141  769373  \n",
       "2012-03-01 00:00:00  59.375000  69.000000  59.250000  69.000000  61.875  \n",
       "2012-03-01 00:05:00  61.111111  64.444444  55.888889  68.444444  62.875  \n",
       "2012-03-01 00:10:00  62.500000  65.625000  61.375000  69.857143  62.000  \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised based on https://github.com/transpaper/gconvRNN/blob/master/model.py\n",
    "\n",
    "def conv(x, ddgf, feat_out, K, W):\n",
    "    '''\n",
    "    x : [batch_size, N_node, feat_in] - input of each time step\n",
    "    nSample : number of samples = batch_size, let's say it is 100\n",
    "    nNode : number of node in graph\n",
    "    feat_in : number of input feature, usually is set as 1\n",
    "    feat_out : number of output feature\n",
    "    ddgf : data driven graph filter\n",
    "    K : size of kernel(number of cheby coefficients), is 1 in the fast graph paper\n",
    "    W : cheby_conv weight [K * feat_in, feat_out]\n",
    "    '''\n",
    "    if len(x.shape) == 2:\n",
    "        x = tf.expand_dims(x, 2) # extend a dimension \"feature_in\"\n",
    "    nSample, nNode, feat_in = x.get_shape()\n",
    "    #feat_in = 1\n",
    "    print (nSample, nNode, feat_in)\n",
    "    nSample, nNode, feat_in = int(nSample), int(nNode), int(feat_in)\n",
    "    \n",
    "    x0 = tf.transpose(x, perm=[1, 2, 0]) #change it to [nNode, feat_in, nSample]\n",
    "    x0 = tf.reshape(x0, [nNode, feat_in*nSample]) # feature_in = 1, [nNode, nSample]\n",
    "    #x = tf.expand_dims(x0, 0) # make it [1, nNode, feat_in*nSample]\n",
    "    \n",
    "    x0 = tf.matmul(ddgf, x0) # graph convolutional #[nNode, nSample]\n",
    "        \n",
    "    #x = tf.reshape(x, [K, nNode, feat_in, nSample])\n",
    "    #x = tf.transpose(x, perm=[3,1,2,0])\n",
    "    x0 = tf.reshape(x0, [nSample*nNode, feat_in*K]) #[nSample*nNode, 1]\n",
    "    \n",
    "    x = tf.matmul(x0, W) #No Bias term?? -> Yes\n",
    "    out = tf.reshape(x, [nSample, nNode, feat_out]) \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-160-cf2d07f7cf52>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-160-cf2d07f7cf52>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "    output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "\n",
    "    \n",
    "    for i in range(num):\n",
    "        Xtem = tf.reshape(x[i,:], [sn, 1]) # 207 by 1\n",
    "        Xtem = tf.transpose(Xtem) # 1 by 207\n",
    "        Xtem = tf.cast(Xtem, dtype=np.float32)\n",
    "        #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "        #Xtem = tf.transpose(Xtem)\n",
    "        #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "        #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "        #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "        #Atem = tf.diag(tf.ones([n_input]))\n",
    "        Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "        Atem1 = normalize_adj(Atem1)\n",
    "        #th = tf.constant(0.01, dtype=tf.float32)\n",
    "        #where = tf.subtract(Atem1, th)\n",
    "        #Atem1 = tf.nn.relu(where)\n",
    "        \n",
    "        Z1 = tf.matmul(Xtem, Atem1) #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "        \n",
    "        #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "        layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        \n",
    "        '''\n",
    "        Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "        Atem2 = normalize_adj(Atem2)\n",
    "        \n",
    "        Z2 = tf.matmul(Atem2, layer_1)\n",
    "        layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        \n",
    "        Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "        Atem3 = normalize_adj(Atem3)\n",
    "        Z3 = tf.matmul(Atem3, layer_2)\n",
    "        layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "        '''\n",
    "        #Atem4 = 0.5*(weights['A4'] + tf.transpose(weights['A4']))#+ Atem \n",
    "        #Atem4 = normalize_adj(Atem4)\n",
    "        #Z4 = tf.matmul(Atem4, layer_3)\n",
    "        #layer_4 = tf.add(tf.matmul(Z4, weights['h4']), biases['b4'])\n",
    "        #layer_4 = tf.nn.relu(layer_4)\n",
    "        \n",
    "        # flattern\n",
    "        #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "        \n",
    "        #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "        #F1 = tf.nn.relu(F1)\n",
    "        \n",
    "        #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "        #F2 = tf.nn.relu(F2)\n",
    "        \n",
    "        #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "        #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        Z4 = layer_1#tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "        #out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "        #out_layer = tf.nn.relu(out_layer)\n",
    "        # weather layer 1\n",
    "        #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "        #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "        #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "        \n",
    "        #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "        #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "        \n",
    "        #print (out_layer.get_shape())\n",
    "        if i ==0:\n",
    "            #print (out_layer.shape)\n",
    "            #tem = tf.reshape(Z4, [1, -1])\n",
    "            output_list = Z4\n",
    "        else:\n",
    "            #tem = tf.reshape(Z4, [1, -1])\n",
    "            output_list = tf.concat([output_list, Z4], 0)\n",
    "        \n",
    "        #print (tf.reduce_mean(tf.pow(output_list-out_layer, 2)))\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    #print ('here!!!!!!!!!!!!!!!!')\n",
    "    #output_list = tf.transpose(output_list)\n",
    "    \n",
    "    #print (output_list.shape)\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    return output_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "   # output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "\n",
    "    #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "    #Xtem = tf.transpose(Xtem)\n",
    "    #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "    #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "    #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "    #Atem = tf.diag(tf.ones([n_input]))\n",
    "    #x = tf.reshape(x, [-1, sn, 1]) # 100, 207, 1\n",
    "    \n",
    "    # x (?, 207, 12)\n",
    "    x = tf.transpose(x, [1, 0, 2]) # 207, ?, 12\n",
    "    x = tf.reshape(x, [sn, -1]) # 207, batch*feature_num\n",
    "    Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "    Atem1 = normalize_adj(Atem1)\n",
    "    #th = tf.constant(0.01, dtype=tf.float32)\n",
    "    #where = tf.subtract(Atem1, th)\n",
    "    #Atem1 = tf.nn.relu(where)\n",
    "\n",
    "    Z1 = tf.matmul(Atem1, x) # 207, batch*feature_num  #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "    Z1 = tf.reshape(Z1, [-1, frequency]) # 207* 100, frequency\n",
    "    #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "    layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) # 207*100, hidden1\n",
    "\n",
    "    \n",
    "    #Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "    #Atem2 = normalize_adj(Atem2)\n",
    "    \n",
    "    #layer_1 = tf.reshape(layer_1, [sn, -1])  # 207, batchsize*hidden1\n",
    "    #Z2 = tf.matmul(Atem2, layer_1)\n",
    "    #Z2 = tf.reshape(Z2, [-1, n_hidden_vec1]) # 207*batchsize, n_hidden_vec1\n",
    "    #layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2) # 207*batchsize, hidden2\n",
    "\n",
    "    #Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "    #Atem3 = normalize_adj(Atem3) \n",
    "    \n",
    "    #layer_2 = tf.reshape(layer_2, [sn, -1])  # 207, batchsize*hidden2\n",
    "    #Z3 = tf.matmul(Atem3, layer_2)\n",
    "    #Z3 = tf.reshape(Z3, [-1, n_hidden_vec2]) # 207*batchsize, hidden2\n",
    "    #layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "    #layer_3 = tf.nn.relu(layer_3) # 207*batchsize, hidden3\n",
    "    \n",
    "    #Atem4 = 0.5*(weights['A4'] + tf.transpose(weights['A4']))#+ Atem \n",
    "    #Atem4 = normalize_adj(Atem4)\n",
    "    #Z4 = tf.matmul(Atem4, layer_3)\n",
    "    #layer_4 = tf.add(tf.matmul(Z4, weights['h4']), biases['b4'])\n",
    "    #layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "    # flattern\n",
    "    #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "\n",
    "    #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "    #F1 = tf.nn.relu(F1)\n",
    "\n",
    "    #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "    #F2 = tf.nn.relu(F2)\n",
    "\n",
    "    #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "    #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    layer_1 = tf.reshape(layer_1, [sn, -1, n_hidden_vec1])\n",
    "    layer_1 = tf.transpose(layer_1, [1, 0, 2]) # batchsize, sn, hidden3\n",
    "    layer_1 = tf.reshape(layer_1, [-1, sn*n_hidden_vec1]) # batchsize, sn*hidden3\n",
    "    Z4 = layer_1\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['outg']), biases['boutg'])\n",
    "    #tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "    #out_layer = tf.nn.relu(out_layer)\n",
    "    # weather layer 1\n",
    "    #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "    #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "    #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "\n",
    "    #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "    #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "\n",
    "\n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, horizon, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "    X_whole = []\n",
    "    Y_whole = []\n",
    "\n",
    "    x_offsets = np.sort(\n",
    "        # np.concatenate(([-week_size + 1, -day_size + 1], np.arange(-11, 1, 1)))\n",
    "        np.concatenate((np.arange(-frequency+1, 1, 1),))\n",
    "    )\n",
    "    # Predict the next one hour\n",
    "    y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "    min_t = abs(min(x_offsets))\n",
    "    max_t = abs(raw_data.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "    for t in range(min_t, max_t):\n",
    "        x_t = raw_data.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "        y_t = raw_data.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "        X_whole.append(x_t)\n",
    "        Y_whole.append(y_t)\n",
    "\n",
    "    X_whole = np.stack(X_whole, axis=0)\n",
    "    time_step = int(X_whole.shape[1] / sn)\n",
    "    X_whole = np.reshape(X_whole, [X_whole.shape[0], sn, time_step])\n",
    "    Y_whole = np.stack(Y_whole, axis=0)\n",
    "    \n",
    "    i = lstm_steps\n",
    "    X_whole_lstm = []\n",
    "    Y_whole_lstm = []\n",
    "    \n",
    "    while i <= X_whole.shape[0]:\n",
    "        X_whole_lstm.append(X_whole[i-lstm_steps:i,:])\n",
    "        Y_whole_lstm.append(Y_whole[i-1])\n",
    "        i = i + 1\n",
    "    \n",
    "    X_whole_lstm = np.stack(X_whole_lstm, axis = 0) # (34239, 10, 207, 12)\n",
    "    Y_whole_lstm = np.stack(Y_whole_lstm, axis = 0) # (34239, 2484)\n",
    "    #print (Y_whole_lstm.shape)\n",
    "    '''\n",
    "    time_step = int(time_step) #\n",
    "\n",
    "    #i = time_step\n",
    "    #X_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn*time_step), dtype = np.float)\n",
    "    #Y_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn), dtype = np.float)\n",
    "\n",
    "    while i < raw_data.shape[0]:\n",
    "        X_whole[i - time_step, ] = raw_data.iloc[(i - time_step):i, 0:sn].values.flatten('F') # 'F' flatten by column, default:flatten by row 0, 1, 2...7\n",
    "        Y_whole[i - time_step, ] = raw_data.iloc[i, 0:sn]\n",
    "        i = i + 1\n",
    "    '''\n",
    "\n",
    "\n",
    "    n_input = sn # station number\n",
    "    n_input_vec = n_input * frequency # 207 * frequency\n",
    "    n_A_vec = n_input * n_input\n",
    "    n_output_vec = Y_whole_lstm.shape[1] # each row represent a result\n",
    "    #print (n_output_vec)\n",
    "\n",
    "    \n",
    "    num_samples = X_whole_lstm.shape[0]\n",
    "    num_test = round(num_samples * 0.2)\n",
    "    num_train = round(num_samples * 0.7)\n",
    "    num_val = num_samples - num_test - num_train\n",
    "    #skip = skip1 + freq_max - time_step#time_step_max - time_step # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "    X_training = X_whole_lstm[:num_train, :]\n",
    "    Y_training = Y_whole_lstm[:num_train, :]\n",
    "    \n",
    "    # shuffle\n",
    "    perm = np.arange(X_training.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    X_training = X_training[perm]\n",
    "    Y_training = Y_training[perm]\n",
    "    \n",
    "    #print (type(X_training))\n",
    "    #X_training = random.Random(6).shuffle(X_training)\n",
    "    #Y_training = random.Random(6).shuffle(Y_training)\n",
    "\n",
    "    X_val = X_whole_lstm[num_train:num_train+num_val, :]\n",
    "    Y_val = Y_whole_lstm[num_train:num_train+num_val, :]\n",
    "    #A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "    X_test = X_whole_lstm[-num_test:, :]\n",
    "    Y_test = Y_whole_lstm[-num_test:, :]\n",
    "\n",
    "    scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "    X_training = scaler.transform(X_training)\n",
    "    Y_training = scaler.transform(Y_training)\n",
    "\n",
    "    X_val = scaler.transform(X_val)\n",
    "    Y_val = scaler.transform(Y_val)\n",
    "\n",
    "    X_test = scaler.transform(X_test)\n",
    "    Y_test = scaler.transform(Y_test)\n",
    "    \n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    test_error_by_h = 0 # test error by horizon\n",
    "    predic_res = []\n",
    "    Y_true = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, lstm_steps, sn, time_step]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    #num = tf.placeholder(tf.int32,[1, 1] )\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1]), dtype=np.float32),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2]), dtype=np.float32),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3]), dtype=np.float32),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_vec3, n_hidden_vec4])),\n",
    "        'outg': tf.Variable(tf.random_normal([sn*n_hidden_vec1, n_hidden_vec4]), dtype=np.float32), \n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, Y_whole.shape[1]]), dtype=np.float32), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A2': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A3': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        #'A4': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([1, n_hidden_vec1]), dtype=np.float32),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        'b2': tf.Variable(tf.random_normal([1, n_hidden_vec2]), dtype=np.float32), #n_hidden_vec2])),\n",
    "        'b3': tf.Variable(tf.random_normal([1, n_hidden_vec3]), dtype=np.float32),#n_hidden_vec3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_input, n_hidden_vec4])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'boutg': tf.Variable(tf.random_normal([1, n_hidden_vec4]), dtype=np.float32), \n",
    "        'bout': tf.Variable(tf.random_normal([Y_whole.shape[1]]), dtype=np.float32), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('lstm'):\n",
    "        lstm = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        rnn_input_seq = tf.unstack(X, lstm_steps, 1) # lstm_steps is the 2nd variable\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq[i] = gcn(rnn_input_seq[i], weights, biases, batch_size,n_input, frequency, 1, n_output_vec)\n",
    "            #print (rnn_input_seq[i].shape)\n",
    "        outputs, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs[-1], [-1, num_hidden])\n",
    "        #print ('123here!!!!!!!!!!!')\n",
    "        pred = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        #print (pred)\n",
    "        #pred = tf.reshape(pred, [-1, Y_whole.shape[1]])\n",
    "        #print ('here!!!!!!!!!!!')\n",
    "        pred = scaler.inverse_transform(pred)\n",
    "        Y_true_tr = scaler.inverse_transform(Y)\n",
    "\n",
    "        cost = masked_mae_tf(pred, Y_true_tr, 0)\n",
    "        #print (cost)\n",
    "        \n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_val = tf.unstack(X, lstm_steps, 1)\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_val[i] = gcn(rnn_input_seq_val[i], weights, biases, batch_size,n_input,frequency, 2, n_output_vec)\n",
    "        outputs_val, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_val, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_val[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_val = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_val = scaler.inverse_transform(pred_val)\n",
    "        Y_true_val = scaler.inverse_transform(Y)\n",
    "        cost_val = masked_mae_tf(pred_val, Y_true_val, 0)\n",
    "        #print ('234here!!!!!!!!!!!')\n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_test = tf.unstack(X, lstm_steps, 1)\n",
    "        \n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_test[i] = gcn(rnn_input_seq_test[i], weights, biases, batch_size,n_input,frequency, 3, n_output_vec)\n",
    "        outputs_test, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_test, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_test[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_tes = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_tes = scaler.inverse_transform(pred_tes)\n",
    "        Y_true_tes = scaler.inverse_transform(Y)\n",
    "        cost_tes = masked_mae_tf(pred_tes, Y_true_tes, 0)\n",
    "        cost_tes_by_horizon = masked_mae_tf_by_horizon(pred_tes, Y_true_tes, 0)\n",
    "        #print ('345here!!!!!!!!!!!')\n",
    "    #rmse\n",
    "    #cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    #optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size) #int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                #print (Y_training[i*batch_size:(i+1)*batch_size,].size())\n",
    "                #num = batch_size\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,],  \n",
    "                                                              keep_prob: keep})\n",
    "                #print (preds)\n",
    "                #print (trueval)\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print ('here!!!!!!!!!!!!!!!!')\n",
    "                avg_cost += c * batch_size #/ total_batch \n",
    "                #Display logs per epoch step\n",
    "                \n",
    "            # rest part of training dataset\n",
    "            #num = num_train - total_batch*batch_size \n",
    "            if total_batch * batch_size != num_train:\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[total_batch*batch_size:num_train,], \n",
    "                                          Y: Y_training[total_batch*batch_size:num_train,],\n",
    "                                                  keep_prob: keep})\n",
    "                avg_cost += c * (num_train - total_batch*batch_size)\n",
    "            \n",
    "            avg_cost = avg_cost / num_train\n",
    "            \n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training MAE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost)) #np.sqrt(avg_cost)\n",
    "                \n",
    "            # also use batch to save memory\n",
    "            # validation\n",
    "            c_val = 0.\n",
    "            total_bat_val = int(num_val/batch_size)\n",
    "            for i in range(total_bat_val):\n",
    "                #num = batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[i*batch_size:(i+1)*batch_size,], \n",
    "                                                          Y: Y_val[i*batch_size:(i+1)*batch_size,],   keep_prob:1})\n",
    "                c_val += c_val_b[0]*batch_size\n",
    "            \n",
    "            if total_bat_val * batch_size != num_val:\n",
    "                #num = num_val - total_bat_val*batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[total_bat_val*batch_size:num_val,], \n",
    "                                                          Y: Y_val[total_bat_val*batch_size:num_val,],  keep_prob:1})\n",
    "                c_val += c_val_b[0] * (num_val - total_bat_val*batch_size)\n",
    "                \n",
    "            c_val = c_val / num_val\n",
    "            \n",
    "            print(\"Validation MAE: \", c_val)\n",
    "            \n",
    "            # test\n",
    "            c_tes = 0.\n",
    "            c_tes_h = []\n",
    "            total_bat_test = int(num_test/batch_size)\n",
    "            \n",
    "            pre_test_tem = [] # save the prediction results\n",
    "            Y_tes_true = []\n",
    "            \n",
    "            for i in range(total_bat_test):\n",
    "                #num = batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch, cost_h = sess.run([cost_tes, pred_tes, Y_true_tes, cost_tes_by_horizon], feed_dict={X: X_test[i*batch_size:(i+1)*batch_size,],\n",
    "                                                                               Y: Y_test[i*batch_size:(i+1)*batch_size,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b*batch_size\n",
    "                c_tes_h.append([i*batch_size for i in cost_h])\n",
    "                #print (cost_h)\n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "                \n",
    "            if total_bat_test * batch_size != num_test:\n",
    "                #num = num_test - total_bat_test*batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch, cost_h= sess.run([cost_tes, pred_tes, Y_true_tes, cost_tes_by_horizon], feed_dict={X: X_test[total_bat_test*batch_size:num_test,],\n",
    "                                                                               Y: Y_test[total_bat_test*batch_size:num_test,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b * (num_test - total_bat_test*batch_size) \n",
    "                c_tes_h.append([i * (num_test - total_bat_test*batch_size) for i in cost_h]) \n",
    "                \n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "                \n",
    "            #print (c_tes_h.shape)   \n",
    "            c_tes_h = np.array(c_tes_h)\n",
    "            \n",
    "            #print (c_tes_h.shape)\n",
    "            pre_test_tem = np.concatenate(pre_test_tem, axis = 0)\n",
    "            #print (pre_test_tem.shape)\n",
    "            Y_tes_true = np.concatenate(Y_tes_true, axis = 0)\n",
    "            \n",
    "            c_tes = c_tes / num_test\n",
    "            c_tes_h = np.sum(c_tes_h, axis = 0)\n",
    "            c_tes_h = c_tes_h / num_test\n",
    "            #c_tes_h = c_tes_h / num_test\n",
    "            \n",
    "            print(\"Test MAE: \", c_tes)\n",
    "            #print(\"predic step: \", cost_by_hor)\n",
    "\n",
    "            if c_val < best_val:\n",
    "                best_val = c_val\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                test_error_by_h = c_tes_h\n",
    "                traing_error = avg_cost#np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pre_test_tem\n",
    "                Y_true = Y_tes_true\n",
    "                #predic_step = cost_by_hor\n",
    "\n",
    "            # early stopping\n",
    "            if c_val >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training error is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation MAE is \", best_val)#(np.sqrt(best_val)))\n",
    "        print(\"The test MAE is \", test_error)#(np.sqrt(test_error)))\n",
    "    \n",
    "    #test_Y = Y_test\n",
    "    #test_error = np.sqrt(test_error)\n",
    "    return test_error, test_error_by_h, predic_res, Y_true#, A1#, predic_step#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increas batch size from 100 to 1000: validation error 39.XX at about 100 epochs\n",
    "# batch size to 50: validation error 6.XX at about 100 epochs\n",
    "# add one more gcn layer: 8.XX\n",
    "# increase learning rate to 0.01, batch size is 100: 10.XX\n",
    "# decrease learning rate to 0.005, batch size is 100: 10.XX\n",
    "# decrease learning rate to 0.005, batch size is 50, two hidden levels (same matrix):\n",
    "    # 6.XX at 100th epoch\n",
    "# decrease learning rate to 0.005, batch size is 50, one hidden level, hidden 40:\n",
    "    # 6.XX at 100th epoch\n",
    "# decrease learning rate to 0.005, batch size is 50, one hidden level, hidden 20:\n",
    "    # drop much faster\n",
    "    # 6.XX can be get, but kind of overfitting at 100th epoch?\n",
    "# decrease learning rate to 0.005, batch size is 50, one hidden level, hidden 10:\n",
    "    # drop much faster\n",
    "    #  at 100th epoch\n",
    "# learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data:\n",
    "    # val: 3.16\n",
    "\n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data, normalize the symmetric adjacency matrix:\n",
    "    # train: 3.24, val: 3.42\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # train: 3.08, val: 3.26\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper\n",
    "    # train: 2.82, val: 2.75\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, one layer\n",
    "    # train: 4.08, val: 3.99, test: 4.29 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, two layers (same adjmatrix)\n",
    "    # train: 4.10, val: 4.19, test: 4.54 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, two layers (different adjmatrix)\n",
    "    # train: 3.52, val: 3.62, test: 4.03\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, two layers (different adjmatrix), bias set as a vector of same length\n",
    "    # train: 3.53, val: 3.58, test: 3.91 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # train: 3.36, val: 3.42, test: 3.82\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # last hidden size 10 to 20\n",
    "    # train: 3.33, val: 3.38, test: 3.71\n",
    "    # train: 3.24, val: 3.31, test: 3.66\n",
    "    # train: 3.22, val: 3.29, test: 3.66\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 5, 5, 10\n",
    "    # train: 3.5, val: 3.48, test: 3.82 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.005\n",
    "    # train: 3.23, val: 3.34, test: 3.74\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 200\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # shuffle\n",
    "    # train: 3.19, val: 3.30, test: 3.67\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # shuffle\n",
    "    # train: 3.15, val: 3.26, test: 3.61\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # shuffle\n",
    "    # fully freedom adjacency matrix (not symmetric )\n",
    "    # train: , val: , test: \n",
    "    # training cost around 6.15, not decreasing anymore\n",
    "    \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.005\n",
    "    # change gradient decent algorithm\n",
    "    # train: 3.19, val: 3.28, test: 3.67\n",
    "\n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.005\n",
    "    # change gradient decent algorithm\n",
    "    # keep: 0.8\n",
    "    # train: 3.30, val: 3.36, test: 3.78\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 200\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # change gradient decent algorithm\n",
    "    # keep: 1\n",
    "    # train: 3.15, val: 3.27, test: 3.63\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 200\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # change gradient decent algorithm\n",
    "    # keep: 1\n",
    "    # bias not the same size\n",
    "    # decreasing slow\n",
    "\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch size 200\n",
    "    # second size 10 to 10\n",
    "    # last hidden size 10 to 30\n",
    "    # set the same size may lead to indecreasing cost (need to increase batch size)\n",
    "    # train: 3.05, val: 3.22, test: 3.73\n",
    "\n",
    "# batch size 150 really slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training MAE= 21.518401722\n",
      "Validation MAE:  19.921693077922736\n",
      "Test MAE:  21.60497831772637\n",
      "Epoch: 0002 Training MAE= 18.676617038\n",
      "Validation MAE:  17.501514184213903\n",
      "Test MAE:  18.812661764551766\n",
      "Epoch: 0003 Training MAE= 15.506689697\n",
      "Validation MAE:  13.915556448219466\n",
      "Test MAE:  14.848728294806996\n",
      "Epoch: 0004 Training MAE= 11.890633290\n",
      "Validation MAE:  10.656445649418517\n",
      "Test MAE:  11.589966264775695\n",
      "Epoch: 0005 Training MAE= 9.407799855\n",
      "Validation MAE:  8.930527798450775\n",
      "Test MAE:  9.912741373117553\n",
      "Epoch: 0006 Training MAE= 8.274435641\n",
      "Validation MAE:  8.193309317540079\n",
      "Test MAE:  9.196068086455663\n",
      "Epoch: 0007 Training MAE= 7.774450738\n",
      "Validation MAE:  7.776701627856624\n",
      "Test MAE:  8.802782792247182\n",
      "Epoch: 0008 Training MAE= 7.487854517\n",
      "Validation MAE:  7.504357344912787\n",
      "Test MAE:  8.51749472214995\n",
      "Epoch: 0009 Training MAE= 7.318429287\n",
      "Validation MAE:  7.332082525657041\n",
      "Test MAE:  8.35904339634253\n",
      "Epoch: 0010 Training MAE= 7.170703825\n",
      "Validation MAE:  7.166229129707726\n",
      "Test MAE:  8.175894759383267\n",
      "Epoch: 0011 Training MAE= 7.071447560\n",
      "Validation MAE:  7.076390217690572\n",
      "Test MAE:  8.082881497926026\n",
      "Epoch: 0012 Training MAE= 6.977328034\n",
      "Validation MAE:  6.998667556874073\n",
      "Test MAE:  8.000714117566204\n",
      "Epoch: 0013 Training MAE= 6.889028046\n",
      "Validation MAE:  6.916620108332947\n",
      "Test MAE:  7.900213721268785\n",
      "Epoch: 0014 Training MAE= 6.797287640\n",
      "Validation MAE:  6.805183396722279\n",
      "Test MAE:  7.7722442428949785\n",
      "Epoch: 0015 Training MAE= 6.679050104\n",
      "Validation MAE:  6.709228487780495\n",
      "Test MAE:  7.654301806007237\n",
      "Epoch: 0016 Training MAE= 6.578577916\n",
      "Validation MAE:  6.627302246372195\n",
      "Test MAE:  7.5519347440797855\n",
      "Epoch: 0017 Training MAE= 6.499342560\n",
      "Validation MAE:  6.565822872802289\n",
      "Test MAE:  7.488053164145845\n",
      "Epoch: 0018 Training MAE= 6.436405287\n",
      "Validation MAE:  6.5419391924447385\n",
      "Test MAE:  7.456398926541516\n",
      "Epoch: 0019 Training MAE= 6.389419767\n",
      "Validation MAE:  6.504994782218097\n",
      "Test MAE:  7.411977302107363\n",
      "Epoch: 0020 Training MAE= 6.347386153\n",
      "Validation MAE:  6.474160570297798\n",
      "Test MAE:  7.377284833298476\n",
      "Epoch: 0021 Training MAE= 6.307074642\n",
      "Validation MAE:  6.43778743883119\n",
      "Test MAE:  7.341676901545659\n",
      "Epoch: 0022 Training MAE= 6.254812701\n",
      "Validation MAE:  6.388156849102383\n",
      "Test MAE:  7.287347810984459\n",
      "Epoch: 0023 Training MAE= 6.209036027\n",
      "Validation MAE:  6.346750329010678\n",
      "Test MAE:  7.245727486289458\n",
      "Epoch: 0024 Training MAE= 6.170112404\n",
      "Validation MAE:  6.3010928996288\n",
      "Test MAE:  7.197013875727654\n",
      "Epoch: 0025 Training MAE= 6.117263831\n",
      "Validation MAE:  6.262954151543387\n",
      "Test MAE:  7.155167142010584\n",
      "Epoch: 0026 Training MAE= 6.112599956\n",
      "Validation MAE:  6.24823109076841\n",
      "Test MAE:  7.150379841685173\n",
      "Epoch: 0027 Training MAE= 6.055731562\n",
      "Validation MAE:  6.211432763259777\n",
      "Test MAE:  7.112365862595319\n",
      "Epoch: 0028 Training MAE= 6.039077600\n",
      "Validation MAE:  6.201554173100604\n",
      "Test MAE:  7.11102233474003\n",
      "Epoch: 0029 Training MAE= 6.012306921\n",
      "Validation MAE:  6.173338221807549\n",
      "Test MAE:  7.0751510718422645\n",
      "Epoch: 0030 Training MAE= 6.012851871\n",
      "Validation MAE:  6.180151723597172\n",
      "Test MAE:  7.085844604825256\n",
      "Epoch: 0031 Training MAE= 5.971206777\n",
      "Validation MAE:  6.12781984788658\n",
      "Test MAE:  7.038012627250592\n",
      "Epoch: 0032 Training MAE= 5.955756922\n",
      "Validation MAE:  6.11720868618819\n",
      "Test MAE:  7.029819245546608\n",
      "Epoch: 0033 Training MAE= 5.926982517\n",
      "Validation MAE:  6.091672298681997\n",
      "Test MAE:  7.005090806097475\n",
      "Epoch: 0034 Training MAE= 5.918918858\n",
      "Validation MAE:  6.075954033510528\n",
      "Test MAE:  6.988849429287759\n",
      "Epoch: 0035 Training MAE= 5.907528582\n",
      "Validation MAE:  6.082945538263251\n",
      "Test MAE:  6.999507809367593\n",
      "Epoch: 0036 Training MAE= 5.878404397\n",
      "Validation MAE:  6.062904782538866\n",
      "Test MAE:  6.973117806159905\n",
      "Epoch: 0037 Training MAE= 5.868028074\n",
      "Validation MAE:  6.050837854399298\n",
      "Test MAE:  6.9600299353285555\n",
      "Epoch: 0038 Training MAE= 5.876093096\n",
      "Validation MAE:  6.049887347395402\n",
      "Test MAE:  6.954564331743661\n",
      "Epoch: 0039 Training MAE= 5.830873279\n",
      "Validation MAE:  6.0327769335168995\n",
      "Test MAE:  6.937703043583588\n",
      "Epoch: 0040 Training MAE= 5.812473789\n",
      "Validation MAE:  6.027568740566282\n",
      "Test MAE:  6.922793330650536\n",
      "Epoch: 0041 Training MAE= 5.809852922\n",
      "Validation MAE:  6.014613677115336\n",
      "Test MAE:  6.904705351365711\n",
      "Epoch: 0042 Training MAE= 5.785127282\n",
      "Validation MAE:  6.028718614230191\n",
      "Test MAE:  6.916528697117591\n",
      "Epoch: 0043 Training MAE= 5.782846917\n",
      "Validation MAE:  5.999821227832432\n",
      "Test MAE:  6.879126937602764\n",
      "Epoch: 0044 Training MAE= 5.748472714\n",
      "Validation MAE:  5.991707516412665\n",
      "Test MAE:  6.868992469000597\n",
      "Epoch: 0045 Training MAE= 5.737465780\n",
      "Validation MAE:  5.980524734859049\n",
      "Test MAE:  6.860507743065158\n",
      "Epoch: 0046 Training MAE= 5.717899915\n",
      "Validation MAE:  5.971931464480658\n",
      "Test MAE:  6.849035634074111\n",
      "Epoch: 0047 Training MAE= 5.708106702\n",
      "Validation MAE:  5.9654175034404675\n",
      "Test MAE:  6.834190792994946\n",
      "Epoch: 0048 Training MAE= 5.692907577\n",
      "Validation MAE:  5.948133374652723\n",
      "Test MAE:  6.8208692647850695\n",
      "Epoch: 0049 Training MAE= 5.677720905\n",
      "Validation MAE:  5.965451306670252\n",
      "Test MAE:  6.834988464280834\n",
      "Epoch: 0050 Training MAE= 5.660662278\n",
      "Validation MAE:  5.940420592788362\n",
      "Test MAE:  6.8084700982367705\n",
      "Epoch: 0051 Training MAE= 5.644271763\n",
      "Validation MAE:  5.938860955899649\n",
      "Test MAE:  6.79406011628485\n",
      "Epoch: 0052 Training MAE= 5.635068290\n",
      "Validation MAE:  5.9209772861787\n",
      "Test MAE:  6.7861929246849835\n",
      "Epoch: 0053 Training MAE= 5.622063722\n",
      "Validation MAE:  5.914331084620343\n",
      "Test MAE:  6.7730996512308455\n",
      "Epoch: 0054 Training MAE= 5.610378165\n",
      "Validation MAE:  5.924824008106315\n",
      "Test MAE:  6.782671771966414\n",
      "Epoch: 0055 Training MAE= 5.606684231\n",
      "Validation MAE:  5.906678927205775\n",
      "Test MAE:  6.770873623392776\n",
      "Epoch: 0056 Training MAE= 5.587391362\n",
      "Validation MAE:  5.890236029659745\n",
      "Test MAE:  6.751514000028637\n",
      "Epoch: 0057 Training MAE= 5.573937286\n",
      "Validation MAE:  5.882877889340811\n",
      "Test MAE:  6.738222125812697\n",
      "Epoch: 0058 Training MAE= 5.563291016\n",
      "Validation MAE:  5.881067540523779\n",
      "Test MAE:  6.743113775151649\n",
      "Epoch: 0059 Training MAE= 5.544283943\n",
      "Validation MAE:  5.878986758907346\n",
      "Test MAE:  6.73519765674537\n",
      "Epoch: 0060 Training MAE= 5.539707761\n",
      "Validation MAE:  5.879571100221063\n",
      "Test MAE:  6.731677793763748\n",
      "Epoch: 0061 Training MAE= 5.526620126\n",
      "Validation MAE:  5.8649388647427525\n",
      "Test MAE:  6.721351894640995\n",
      "Epoch: 0062 Training MAE= 5.520329224\n",
      "Validation MAE:  5.860698856576516\n",
      "Test MAE:  6.7105084894207865\n",
      "Epoch: 0063 Training MAE= 5.513373554\n",
      "Validation MAE:  5.853503474353873\n",
      "Test MAE:  6.704263231740031\n",
      "Epoch: 0064 Training MAE= 5.488034907\n",
      "Validation MAE:  5.826794050035685\n",
      "Test MAE:  6.675949535085818\n",
      "Epoch: 0065 Training MAE= 5.484414713\n",
      "Validation MAE:  5.819773256343646\n",
      "Test MAE:  6.66508433083963\n",
      "Epoch: 0066 Training MAE= 5.469195864\n",
      "Validation MAE:  5.814739718054333\n",
      "Test MAE:  6.664897835155458\n",
      "Epoch: 0067 Training MAE= 5.458520137\n",
      "Validation MAE:  5.810107217217884\n",
      "Test MAE:  6.667137312460928\n",
      "Epoch: 0068 Training MAE= 5.451692658\n",
      "Validation MAE:  5.859592869333977\n",
      "Test MAE:  6.713848053721516\n",
      "Epoch: 0069 Training MAE= 5.434965351\n",
      "Validation MAE:  5.829219309953007\n",
      "Test MAE:  6.6856322794660965\n",
      "Epoch: 0070 Training MAE= 5.420273519\n",
      "Validation MAE:  5.811692693807783\n",
      "Test MAE:  6.6647557959554495\n",
      "Epoch: 0071 Training MAE= 5.404928942\n",
      "Validation MAE:  5.783603584679374\n",
      "Test MAE:  6.630994810607594\n",
      "Epoch: 0072 Training MAE= 5.398518232\n",
      "Validation MAE:  5.773316459934207\n",
      "Test MAE:  6.622325117874535\n",
      "Epoch: 0073 Training MAE= 5.385986195\n",
      "Validation MAE:  5.7725014129694365\n",
      "Test MAE:  6.630086438745317\n",
      "Epoch: 0074 Training MAE= 5.368826018\n",
      "Validation MAE:  5.797627598699862\n",
      "Test MAE:  6.648602601228308\n",
      "Epoch: 0075 Training MAE= 5.366001045\n",
      "Validation MAE:  5.776678579567123\n",
      "Test MAE:  6.636028628294303\n",
      "Epoch: 0076 Training MAE= 5.355687111\n",
      "Validation MAE:  5.747392525638107\n",
      "Test MAE:  6.604554738453974\n",
      "Epoch: 0077 Training MAE= 5.336064865\n",
      "Validation MAE:  5.741372623582826\n",
      "Test MAE:  6.608147034037286\n",
      "Epoch: 0078 Training MAE= 5.320818103\n",
      "Validation MAE:  5.743154637134858\n",
      "Test MAE:  6.612439691663607\n",
      "Epoch: 0079 Training MAE= 5.318003711\n",
      "Validation MAE:  5.783069269500509\n",
      "Test MAE:  6.644720175402376\n",
      "Epoch: 0080 Training MAE= 5.306289176\n",
      "Validation MAE:  5.780305121066797\n",
      "Test MAE:  6.660814800268021\n",
      "Epoch: 0081 Training MAE= 5.294300028\n",
      "Validation MAE:  5.768299774531901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:  6.642401097829994\n",
      "Epoch: 0082 Training MAE= 5.281252617\n",
      "Validation MAE:  5.780485393357103\n",
      "Test MAE:  6.6748591147473055\n",
      "Epoch: 0083 Training MAE= 5.279870369\n",
      "Validation MAE:  5.762150886285044\n",
      "Test MAE:  6.662103708163894\n",
      "Epoch: 0084 Training MAE= 5.261436736\n",
      "Validation MAE:  5.742778388253094\n",
      "Test MAE:  6.627832292205035\n",
      "Epoch: 0085 Training MAE= 5.249268584\n",
      "Validation MAE:  5.75151046001128\n",
      "Test MAE:  6.643908103341899\n",
      "Epoch: 0086 Training MAE= 5.236140807\n",
      "Validation MAE:  5.7413709076651696\n",
      "Test MAE:  6.63397038412853\n",
      "Epoch: 0087 Training MAE= 5.228285209\n",
      "Validation MAE:  5.745785521764825\n",
      "Test MAE:  6.637634966526263\n",
      "Epoch: 0088 Training MAE= 5.218562703\n",
      "Validation MAE:  5.737835713546642\n",
      "Test MAE:  6.635852259743003\n",
      "Epoch: 0089 Training MAE= 5.205925330\n",
      "Validation MAE:  5.71642729139676\n",
      "Test MAE:  6.60848783405876\n",
      "Epoch: 0090 Training MAE= 5.194510459\n",
      "Validation MAE:  5.713136982743758\n",
      "Test MAE:  6.608851981974115\n",
      "Epoch: 0091 Training MAE= 5.187428697\n",
      "Validation MAE:  5.713733481664727\n",
      "Test MAE:  6.608961469745232\n",
      "Epoch: 0092 Training MAE= 5.182492319\n",
      "Validation MAE:  5.7085992297987\n",
      "Test MAE:  6.609020115145977\n",
      "Epoch: 0093 Training MAE= 5.169441381\n",
      "Validation MAE:  5.717584846663649\n",
      "Test MAE:  6.63375759960213\n",
      "Epoch: 0094 Training MAE= 5.171662978\n",
      "Validation MAE:  5.720273710515377\n",
      "Test MAE:  6.636047202455752\n",
      "Epoch: 0095 Training MAE= 5.164056805\n",
      "Validation MAE:  5.719823705018872\n",
      "Test MAE:  6.650619580216922\n",
      "Epoch: 0096 Training MAE= 5.156696962\n",
      "Validation MAE:  5.711485622573073\n",
      "Test MAE:  6.622891003136252\n",
      "Epoch: 0097 Training MAE= 5.150829225\n",
      "Validation MAE:  5.691520356783902\n",
      "Test MAE:  6.606341280681685\n",
      "Epoch: 0098 Training MAE= 5.150764177\n",
      "Validation MAE:  5.682566639280667\n",
      "Test MAE:  6.59745338388143\n",
      "Epoch: 0099 Training MAE= 5.143008942\n",
      "Validation MAE:  5.686335838624161\n",
      "Test MAE:  6.612790484727082\n",
      "Epoch: 0100 Training MAE= 5.189935074\n",
      "Validation MAE:  5.729069901208808\n",
      "Test MAE:  6.678009719391916\n",
      "Epoch: 0101 Training MAE= 5.146021247\n",
      "Validation MAE:  5.656712713032744\n",
      "Test MAE:  6.572189679997283\n",
      "Epoch: 0102 Training MAE= 5.115214878\n",
      "Validation MAE:  5.656680806709902\n",
      "Test MAE:  6.573837943696022\n",
      "Epoch: 0103 Training MAE= 5.106482505\n",
      "Validation MAE:  5.658929755217837\n",
      "Test MAE:  6.6016973230086515\n",
      "Epoch: 0104 Training MAE= 5.095524272\n",
      "Validation MAE:  5.651652524071018\n",
      "Test MAE:  6.586865439869606\n",
      "Epoch: 0105 Training MAE= 5.085566543\n",
      "Validation MAE:  5.650240024510961\n",
      "Test MAE:  6.588981271886637\n",
      "Epoch: 0106 Training MAE= 5.076116201\n",
      "Validation MAE:  5.641733075580458\n",
      "Test MAE:  6.576485804840324\n",
      "Epoch: 0107 Training MAE= 5.071847401\n",
      "Validation MAE:  5.634716225366523\n",
      "Test MAE:  6.592608433323962\n",
      "Epoch: 0108 Training MAE= 5.079697149\n",
      "Validation MAE:  5.68130351157084\n",
      "Test MAE:  6.653891555478684\n",
      "Epoch: 0109 Training MAE= 5.173816069\n",
      "Validation MAE:  5.805709706605786\n",
      "Test MAE:  6.9077480169678624\n",
      "Epoch: 0110 Training MAE= 5.093184197\n",
      "Validation MAE:  5.646193264174635\n",
      "Test MAE:  6.610576285151848\n",
      "Epoch: 0111 Training MAE= 5.050548762\n",
      "Validation MAE:  5.636798249544018\n",
      "Test MAE:  6.595879300630324\n",
      "Epoch: 0112 Training MAE= 5.038867660\n",
      "Validation MAE:  5.619754105588815\n",
      "Test MAE:  6.57359678398207\n",
      "Epoch: 0113 Training MAE= 5.036211826\n",
      "Validation MAE:  5.649228301361529\n",
      "Test MAE:  6.623888048608384\n",
      "Epoch: 0114 Training MAE= 5.031675444\n",
      "Validation MAE:  5.67264305936159\n",
      "Test MAE:  6.653128991285055\n",
      "Epoch: 0115 Training MAE= 5.030429437\n",
      "Validation MAE:  5.67105518814421\n",
      "Test MAE:  6.669809655423303\n",
      "Epoch: 0116 Training MAE= 5.017122338\n",
      "Validation MAE:  5.684885056349483\n",
      "Test MAE:  6.697084795177235\n",
      "Epoch: 0117 Training MAE= 5.009440316\n",
      "Validation MAE:  5.672497188957938\n",
      "Test MAE:  6.682679191090309\n",
      "Epoch: 0118 Training MAE= 5.004655307\n",
      "Validation MAE:  5.657473463211616\n",
      "Test MAE:  6.673246644329617\n",
      "Epoch: 0119 Training MAE= 4.998201716\n",
      "Validation MAE:  5.642085339901221\n",
      "Test MAE:  6.642169039655661\n",
      "Epoch: 0120 Training MAE= 4.992767269\n",
      "Validation MAE:  5.6435158130896355\n",
      "Test MAE:  6.646342120182429\n",
      "Epoch: 0121 Training MAE= 4.987774819\n",
      "Validation MAE:  5.6574517340555674\n",
      "Test MAE:  6.664364028314654\n",
      "Epoch: 0122 Training MAE= 4.986127780\n",
      "Validation MAE:  5.675964191882279\n",
      "Test MAE:  6.705042394865584\n",
      "Epoch: 0123 Training MAE= 5.043605412\n",
      "Validation MAE:  5.6682893760012885\n",
      "Test MAE:  6.677918968557146\n",
      "Epoch: 0124 Training MAE= 4.981369631\n",
      "Validation MAE:  5.655229833004248\n",
      "Test MAE:  6.691457291975006\n",
      "Epoch: 0125 Training MAE= 4.966365964\n",
      "Validation MAE:  5.678228754196724\n",
      "Test MAE:  6.724765895248104\n",
      "Epoch: 0126 Training MAE= 4.959175181\n",
      "Validation MAE:  5.6426362051580945\n",
      "Test MAE:  6.689163406568125\n",
      "Epoch: 0127 Training MAE= 4.951153069\n",
      "Validation MAE:  5.648360141002349\n",
      "Test MAE:  6.6898860853420485\n",
      "Epoch: 0128 Training MAE= 4.947744146\n",
      "Validation MAE:  5.6583637112248555\n",
      "Test MAE:  6.697995093951766\n",
      "Epoch: 0129 Training MAE= 4.942518305\n",
      "Validation MAE:  5.656984893074871\n",
      "Test MAE:  6.710750593340541\n",
      "Epoch: 0130 Training MAE= 4.935824157\n",
      "Validation MAE:  5.647466516842807\n",
      "Test MAE:  6.716752647152607\n",
      "Epoch: 0131 Training MAE= 4.928333367\n",
      "Validation MAE:  5.644755081538737\n",
      "Test MAE:  6.7206287986954765\n",
      "Epoch: 0132 Training MAE= 4.926018589\n",
      "Validation MAE:  5.639046665525784\n",
      "Test MAE:  6.718906528324189\n",
      "Epoch: 0133 Training MAE= 4.916572573\n",
      "Validation MAE:  5.631216644370643\n",
      "Test MAE:  6.704942643651128\n",
      "Epoch: 0134 Training MAE= 4.913573093\n",
      "Validation MAE:  5.626585521837221\n",
      "Test MAE:  6.702599901825098\n",
      "Epoch: 0135 Training MAE= 4.912260354\n",
      "Validation MAE:  5.614146587622427\n",
      "Test MAE:  6.667986400702832\n",
      "Epoch: 0136 Training MAE= 4.912096013\n",
      "Validation MAE:  5.632147865573855\n",
      "Test MAE:  6.707781316868875\n",
      "Epoch: 0137 Training MAE= 4.897824381\n",
      "Validation MAE:  5.623356491979892\n",
      "Test MAE:  6.705268511059929\n",
      "Epoch: 0138 Training MAE= 4.896491676\n",
      "Validation MAE:  5.6307872994972845\n",
      "Test MAE:  6.725042522066717\n",
      "Epoch: 0139 Training MAE= 4.892672058\n",
      "Validation MAE:  5.605613524026245\n",
      "Test MAE:  6.6622011762828715\n",
      "Epoch: 0140 Training MAE= 4.883626755\n",
      "Validation MAE:  5.655965714559068\n",
      "Test MAE:  6.759333712181952\n",
      "Epoch: 0141 Training MAE= 4.942164724\n",
      "Validation MAE:  5.688787536899539\n",
      "Test MAE:  6.806668682713842\n",
      "Epoch: 0142 Training MAE= 4.927537015\n",
      "Validation MAE:  5.654014549116148\n",
      "Test MAE:  6.779091177795696\n",
      "Epoch: 0143 Training MAE= 4.901569286\n",
      "Validation MAE:  5.62155775432169\n",
      "Test MAE:  6.697833452490685\n",
      "Epoch: 0144 Training MAE= 4.842329056\n",
      "Validation MAE:  5.608681181051435\n",
      "Test MAE:  6.681191535740286\n",
      "Epoch: 0145 Training MAE= 4.801115050\n",
      "Validation MAE:  5.599928198069551\n",
      "Test MAE:  6.644921140716726\n",
      "Epoch: 0146 Training MAE= 4.782305097\n",
      "Validation MAE:  5.622838288328073\n",
      "Test MAE:  6.706946223439013\n",
      "Epoch: 0147 Training MAE= 4.767344779\n",
      "Validation MAE:  5.599707913224715\n",
      "Test MAE:  6.68610101575346\n",
      "Epoch: 0148 Training MAE= 4.753767515\n",
      "Validation MAE:  5.607018516011482\n",
      "Test MAE:  6.71047940200603\n",
      "Epoch: 0149 Training MAE= 4.745612684\n",
      "Validation MAE:  5.577583699330797\n",
      "Test MAE:  6.64227055340618\n",
      "Epoch: 0150 Training MAE= 4.727176218\n",
      "Validation MAE:  5.580466005923975\n",
      "Test MAE:  6.6538593728344955\n",
      "Epoch: 0151 Training MAE= 4.708693489\n",
      "Validation MAE:  5.598168282613267\n",
      "Test MAE:  6.679304872886823\n",
      "Epoch: 0152 Training MAE= 4.691336422\n",
      "Validation MAE:  5.59731060571044\n",
      "Test MAE:  6.674674364820434\n",
      "Epoch: 0153 Training MAE= 4.689884427\n",
      "Validation MAE:  5.600523238634541\n",
      "Test MAE:  6.702186151566619\n",
      "Epoch: 0154 Training MAE= 4.658735886\n",
      "Validation MAE:  5.596447311178611\n",
      "Test MAE:  6.693651959467568\n",
      "Epoch: 0155 Training MAE= 4.663188016\n",
      "Validation MAE:  5.615518117473073\n",
      "Test MAE:  6.759277413789156\n",
      "Epoch: 0156 Training MAE= 4.654389323\n",
      "Validation MAE:  5.61858518280252\n",
      "Test MAE:  6.7557446853942915\n",
      "Epoch: 0157 Training MAE= 4.644485492\n",
      "Validation MAE:  5.596475733457691\n",
      "Test MAE:  6.715805074875852\n",
      "Epoch: 0158 Training MAE= 4.639231106\n",
      "Validation MAE:  5.602046538443461\n",
      "Test MAE:  6.725051746348914\n",
      "Epoch: 0159 Training MAE= 4.628655305\n",
      "Validation MAE:  5.599415949661366\n",
      "Test MAE:  6.757014713351405\n",
      "Epoch: 0160 Training MAE= 4.622567168\n",
      "Validation MAE:  5.572397357355939\n",
      "Test MAE:  6.690883029654246\n",
      "Epoch: 0161 Training MAE= 4.608163819\n",
      "Validation MAE:  5.572640380720152\n",
      "Test MAE:  6.678383712125252\n",
      "Epoch: 0162 Training MAE= 4.611302915\n",
      "Validation MAE:  5.561596501482664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:  6.6483484910933495\n",
      "Epoch: 0163 Training MAE= 4.598157677\n",
      "Validation MAE:  5.577320687092134\n",
      "Test MAE:  6.712279430878982\n",
      "Epoch: 0164 Training MAE= 4.597936612\n",
      "Validation MAE:  5.585733907936263\n",
      "Test MAE:  6.684950896085031\n",
      "Epoch: 0165 Training MAE= 4.596115592\n",
      "Validation MAE:  5.584982137610442\n",
      "Test MAE:  6.709264383749085\n",
      "Epoch: 0166 Training MAE= 4.586820770\n",
      "Validation MAE:  5.590192029075901\n",
      "Test MAE:  6.746519266140515\n",
      "Epoch: 0167 Training MAE= 4.583685234\n",
      "Validation MAE:  5.590391190382686\n",
      "Test MAE:  6.732541916961826\n",
      "Epoch: 0168 Training MAE= 4.580125476\n",
      "Validation MAE:  5.5571977309066884\n",
      "Test MAE:  6.681639597805456\n",
      "Epoch: 0169 Training MAE= 4.576993147\n",
      "Validation MAE:  5.606893790029261\n",
      "Test MAE:  6.770372802627437\n",
      "Epoch: 0170 Training MAE= 4.613206274\n",
      "Validation MAE:  5.58911906541699\n",
      "Test MAE:  6.730845081386714\n",
      "Epoch: 0171 Training MAE= 4.574765857\n",
      "Validation MAE:  5.591009512434911\n",
      "Test MAE:  6.727227707468936\n",
      "Epoch: 0172 Training MAE= 4.585427332\n",
      "Validation MAE:  5.549014815448844\n",
      "Test MAE:  6.689962408876468\n",
      "Epoch: 0173 Training MAE= 4.552820435\n",
      "Validation MAE:  5.574913449531054\n",
      "Test MAE:  6.77081846804458\n",
      "Epoch: 0174 Training MAE= 4.552159261\n",
      "Validation MAE:  5.577067131543681\n",
      "Test MAE:  6.752229576790345\n",
      "Epoch: 0175 Training MAE= 4.543715961\n",
      "Validation MAE:  5.5727371891049575\n",
      "Test MAE:  6.760054669496456\n",
      "Epoch: 0176 Training MAE= 4.547711428\n",
      "Validation MAE:  5.571433384053028\n",
      "Test MAE:  6.813030148513266\n",
      "Epoch: 0177 Training MAE= 4.533426427\n",
      "Validation MAE:  5.5712742422619\n",
      "Test MAE:  6.799727603769212\n",
      "Epoch: 0178 Training MAE= 4.530902477\n",
      "Validation MAE:  5.582639339196421\n",
      "Test MAE:  6.818116960707921\n",
      "Epoch: 0179 Training MAE= 4.520385655\n",
      "Validation MAE:  5.595303114313278\n",
      "Test MAE:  6.8618372404622345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6f9ec51c6191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     test_error, test_error_by_h, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n\u001b[0;32m---> 70\u001b[0;31m                                                                 n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;31m#val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m#print (\"finished A running: \", i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0bb49fc0cd1d>\u001b[0m in \u001b[0;36mgcn_corr_final\u001b[0;34m(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n\u001b[1;32m    267\u001b[0m                                                       \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                                                               keep_prob: keep})\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;31m#print (preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;31m#print (trueval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# change hidden number\n",
    "# change batch_size\n",
    "# set size\n",
    "#from bayes_opt import BayesianOptimization\n",
    "import datetime\n",
    "#freq_max = 12\n",
    "#time_step = 12\n",
    "learning_rate = 0.002\n",
    "decay = 0.9 \n",
    "batch_size = 500\n",
    "num_hidden = 100 # number of hiddent units in LSTM Cell\n",
    "early_stop_th = 150\n",
    "training_epochs = 500\n",
    "keep = 1#0.2\n",
    "#time_step_max = 10\n",
    "\n",
    "sn = 207 # station num\n",
    "#test = 2000 \n",
    "reg1 = 0.05#0.05\n",
    "reg2 = 0.05#0.1\n",
    "reg3 = 0.05\n",
    "frequency = 12\n",
    "horizon = 12\n",
    "lstm_steps = 3  # number of lstm cells\n",
    "n_hidden_vec1 = 5\n",
    "n_hidden_vec2 = 5\n",
    "n_hidden_vec3 = 10\n",
    "n_hidden_vec4 = 10\n",
    "#num = 0\n",
    "#All_pred = np.empty([2000, 207])\n",
    "#All_Y = np.empty([2000, 207])\n",
    "\n",
    "#24*90\n",
    "#step = 0\n",
    "#gap = 100\n",
    "#training = 0.7\n",
    "#validation = 0.1\n",
    "#test = 0.2\n",
    "\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "\n",
    "\n",
    "rep = 1 # repeating times\n",
    "\n",
    "#total_sn = 0\n",
    "#num_iter = 50\n",
    "#init_points = 200\n",
    "\n",
    "\n",
    "# stdbscan\n",
    "#spatial_threshold = 300\n",
    "#temporal_threshold = 300\n",
    "#min_neighbors = 1 # number of neighbor\n",
    "\n",
    "#frequency2 = skip1 + freq_max + training\n",
    "\n",
    "#while step < 2000:\n",
    "\n",
    "#hourly_bike_cluster = hourly_bike\n",
    "best = -10000\n",
    "pre_best = []\n",
    "test_Y_best = []\n",
    "test_error_best = 1000\n",
    "A1_best = []\n",
    "# A2_best = []\n",
    "for i in range(rep):\n",
    "    a = datetime.datetime.now()\n",
    "    test_error, test_error_by_h, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n",
    "                                                                n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "    #print (\"finished A running: \", i)\n",
    "    b = datetime.datetime.now()\n",
    "    print(b-a)\n",
    "    \n",
    "\n",
    "    #total_sn = total_sn + sn\n",
    "\n",
    "    #total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", c)\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "    #step = step + gap\n",
    "    #skip1 = skip1 + gap\n",
    "    \n",
    "    #np.savetxt(\"prediction_2.csv\", All_pred, delimiter = ',')\n",
    "    #np.savetxt(\"prediction_Y_2.csv\", All_Y, delimiter = ',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.97430119, 2.94902541, 2.92731638, 2.90907956, 2.89517942,\n",
       "       2.88547814, 2.8794575 , 2.8779491 , 2.88067882, 2.88815727,\n",
       "       2.90016863, 2.91680584])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error_by_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../../data/trajectory/lstm_gcnn_prediction_2.9168.csv\", predic_res, delimiter = ',')\n",
    "np.savetxt(\"../../data/trajectory/lstm_gcnn_prediction_2.9168_Y.csv\", Y_true, delimiter = ',')\n",
    "np.savetxt(\"../../data/trajectory/lstm_gcnn_prediction_2.9168by_horizon.csv\", test_error_by_h, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase batchsize from 500 to 1000,\n",
    "    #no luck\n",
    "# decrease batchsize from 500 to 100, \n",
    "    # 0/2\n",
    "# batchsize 500\n",
    "    # 1/2 low, 3.01, 2.87, 3.16\n",
    "    # hidden state 100 1/3 3.07, 2.94, 3.21\n",
    "    # reduce learning rate from 0.01 to 0.005: 3/4 3.20, 3.06, 3.34\n",
    "                                                   3.20, 3.08, 3.34\n",
    "                                        500 epochs 3.00, 2.88, 3.12\n",
    "    # hidden 3 from 20 to 10: 4/4\n",
    "    # increase lstm steps from 1 to 3: 1/1: 2.85, 2.73, 3.00\n",
    "    # hidden 1 and 2 are 5, hidden 3 10: 1/1: 2.86, 2.75, 2.99\n",
    "    # hidden 4 10, no output layer for gcn: 1/1: 2.74, 2.66, 2.93\n",
    "    # reduce learning rate from 0.005 to 0.002: 6/6: 2.80, 2.73, 2.93\n",
    "                                                     2.79, 2.71, 2.92\n",
    "                                                     2.74, 2.68, 2.89\n",
    "                                                     2.82, 2.74, 2.96\n",
    "                                                     2.83, 2.73, 2.98\n",
    "                                                     2.77, 2.69, 2.94\n",
    "                                                     2.78, 2.68, 2.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
